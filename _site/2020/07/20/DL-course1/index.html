<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="google-site-verification" content="xBT4GhYoi5qRD5tr338pgPM5OWHHIDR6mNg1a3euekI" />
    <meta name="viewport" content="width=device-width, initial-scale=1, viewport-fit=cover">
    <meta name="description" content="Yuwei's blog">
    <meta name="keywords"  content="tech, astronomy, astrophysics, CV, ML, DL">
    <meta name="theme-color" content="#000000">
    
    <!-- Open Graph -->
    <meta property="og:title" content="Neural Networks and Deep Learning - Yuwei的博客 | Yuwei Blog">
    
    <meta property="og:type" content="article">
    <meta property="og:description" content="
  Hi,

  Just a quick reminder: this is a quite personalized lecture notes and is not meant for recording every details of the course.

  Still, hope it can be of help.

  Comments and corrections...">
    
    <meta property="article:published_time" content="2020-07-20T00:00:00Z">
    
    
    <meta property="article:author" content="Yuwei">
    
    
    <meta property="article:tag" content="deep learning">
    
    <meta property="article:tag" content="Coursera">
    
    <meta property="article:tag" content="note">
    
    
    <meta property="og:image" content="http://localhost:4000/img/avatar-yw.jpg">
    <meta property="og:url" content="http://localhost:4000/2020/07/20/DL-course1/">
    <meta property="og:site_name" content="Yuwei的博客 | Yuwei Blog">
    
    <title>Neural Networks and Deep Learning - Yuwei的博客 | Yuwei Blog</title>

    <!-- Web App Manifest -->
    <link rel="manifest" href="/pwa/manifest.json">

    <!-- Favicon -->
    <link rel="shortcut icon" href="/img/favicon.ico">
    
    <!-- Canonical URL -->
    <link rel="canonical" href="http://localhost:4000/2020/07/20/DL-course1/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/hux-blog.min.css">

    <!-- Custom Fonts -->
    <!-- <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.3.0/css/font-awesome.min.css" rel="stylesheet" type="text/css"> -->
    <!-- Hux change font-awesome CDN to qiniu -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">


    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- ga & ba script hoook -->
    <script></script>
</head>


<!-- hack iOS CSS :active style -->
<body ontouchstart="">

    <!-- Navigation -->

    <nav class="navbar navbar-default navbar-custom navbar-fixed-top">
        
        <div class="container-fluid">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand" href="/">Yuwei Blog</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div id="huxblog_navbar">
                <div class="navbar-collapse">
                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="/">Home</a>
                        </li>
                        
                        
                        
                        
                        <li>
                            <a href="/about/">About</a>
                        </li>
                        
                        
                        
                        <li>
                            <a href="/archive/">Archive</a>
                        </li>
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        
                        <li class="search-icon">
                            <a href="javascript:void(0)">
                                <i class="fa fa-search"></i>
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <script>
        // Drop Bootstarp low-performance Navbar
        // Use customize navbar with high-quality material design animation
        // in high-perf jank-free CSS3 implementation
        var $body = document.body;
        var $toggle = document.querySelector('.navbar-toggle');
        var $navbar = document.querySelector('#huxblog_navbar');
        var $collapse = document.querySelector('.navbar-collapse');

        var __HuxNav__ = {
            close: function () {
                $navbar.className = " ";
                // wait until animation end.
                setTimeout(function () {
                    // prevent frequently toggle
                    if ($navbar.className.indexOf('in') < 0) {
                        $collapse.style.height = "0px"
                    }
                }, 400)
            },
            open: function () {
                $collapse.style.height = "auto"
                $navbar.className += " in";
            }
        }

        // Bind Event
        $toggle.addEventListener('click', function (e) {
            if ($navbar.className.indexOf('in') > 0) {
                __HuxNav__.close()
            } else {
                __HuxNav__.open()
            }
        })

        /**
         * Since Fastclick is used to delegate 'touchstart' globally
         * to hack 300ms delay in iOS by performing a fake 'click',
         * Using 'e.stopPropagation' to stop 'touchstart' event from 
         * $toggle/$collapse will break global delegation.
         * 
         * Instead, we use a 'e.target' filter to prevent handler
         * added to document close HuxNav.  
         *
         * Also, we use 'click' instead of 'touchstart' as compromise
         */
        document.addEventListener('click', function (e) {
            if (e.target == $toggle) return;
            if (e.target.className == 'icon-bar') return;
            __HuxNav__.close();
        })
    </script>
    <!-- Search -->
<div class="search-page">
  <div class="search-icon-close-container">
    <span class="search-icon-close">
      <i class="fa fa-chevron-down"></i>
    </span>
  </div>
  <div class="search-main container">
    <div class="row">
      <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
        <form></form>
        <input type="text" id="search-input" placeholder="$ grep...">
        </form>
        <div id="search-results" class="mini-post-list"></div>
      </div>
    </div>
  </div>
</div>

    <!-- Image to hack wechat -->
<!-- <img src="/img/icon_wechat.png" width="0" height="0"> -->
<!-- <img src="/img/post-bg-os-metro.jpg" width="0" height="0"> -->

<!-- Post Header -->



<style type="text/css">
    header.intro-header{
        position: relative;
        background-image: url('/img/post-bg-os-metro.jpg');
        background: ;
    }

    
    header.intro-header .header-mask{
        width: 100%;
        height: 100%;
        position: absolute;
        background: rgba(0,0,0, 0.3);
    }
    
</style>

<header class="intro-header" >

    <div class="header-mask"></div>
    
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="post-heading">
                    <div class="tags">
                        
                        <a class="tag" href="/archive/?tag=deep+learning" title="deep learning">deep learning</a>
                        
                        <a class="tag" href="/archive/?tag=Coursera" title="Coursera">Coursera</a>
                        
                        <a class="tag" href="/archive/?tag=note" title="note">note</a>
                        
                    </div>
                    <h1>Neural Networks and Deep Learning</h1>
                    
                    <h2 class="subheading">Notes for Coursera Deep Learning Specialization -- Course 1</h2>
                    <span class="meta">Posted by Yuwei on July 20, 2020</span>
                </div>
            </div>
        </div>
    </div>
</header>






<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">

    <!-- Post Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                post-container">

                <!-- Multi-Lingual -->
                

				<blockquote>
  <p>Hi,</p>

  <p>Just a quick reminder: this is a quite personalized lecture notes and is not meant for recording every details of the course.</p>

  <p>Still, hope it can be of help.</p>

  <p>Comments and corrections are welcomed : )</p>
</blockquote>

<h1 id="week-2">Week 2</h1>

<h3 id="logistic-regression">Logistic regression</h3>

<blockquote>
  <p>A  <strong>binary classification</strong> method in <strong>supervised learning</strong></p>
</blockquote>

<h4 id="general-idea">General idea</h4>

<ul>
  <li>input: $x \in \mathbb{R}^{n_x \times m}$</li>
  <li>hidden layer: logistic regression neurons with parameters $w \in \mathbb{R}^{n_x}$ and $b \in \mathbb{R}$
    <ul>
      <li>$z=w^Tx+b$</li>
      <li>binary classification: $\hat{y} = a = \sigma(z)$, $\sigma$ is the <strong>sigmoid function</strong>, $1\over{1+e^{-z}}$</li>
    </ul>
  </li>
  <li>output: prediction $\hat{y}=$ 0 or 1</li>
</ul>

<p><strong>Now, use gradient descent to train the parameters $w$ and $ b$ :</strong></p>

<ul>
  <li>define <strong>loss function</strong>: $L(\hat{y}, y) = -(y\log{\hat{y}}+(1-y)\log{1-\hat{y}})$
    <ul>
      <li>applies for <strong>one</strong> case</li>
      <li>$y$ can only be 0 or 1</li>
    </ul>
  </li>
  <li>define <strong>cost function</strong>: $J(w, b) = {1\over{m}}\sum\limits_{i=1}^{m}L(a^{(i)}, y^{(i)})$
    <ul>
      <li>applies for all ($m$) the cases</li>
    </ul>
  </li>
  <li>repeatedly update the parameters with:
    <ul>
      <li>$w:= w-\alpha \frac{\mathrm{d}J(w)}{\mathrm{d} w}$</li>
      <li>$b:= b-\alpha \frac{\mathrm{d}J(b)}{\mathrm{d} b}$</li>
      <li>where $\alpha$ is the <strong>learning rate</strong></li>
      <li>when coding, a variable’s (e.g. $w$) derivative of the final product (e.g. $J$ in this case), $\frac{\mathrm{d}J(w)}{\mathrm{d} w}$  is written as “$\mathrm{d} w$”</li>
      <li>the derivatives are calculated by the <strong>“backpropagation”</strong> method, basically just get the derivatives start from the output value according to the chain rule</li>
      <li>the process of <strong>minimizing</strong> the cost function $J$ to minimum, is to <strong>maximize</strong> the likelihood of $\hat{y}$ under the assumption of IID (independent identical distribution)</li>
    </ul>
  </li>
</ul>

<p><strong>More details about  backpropagation:</strong></p>

<ul>
  <li>basically just get the derivatives start from the output value based on the chain rule</li>
  <li>$\mathrm{d} z = \frac{1}{m}(a - y) $</li>
  <li>$\mathrm{d}w = \frac{1}{m}x\mathrm{d} z$</li>
  <li>$\mathrm{d}b = \frac{1}{m}\mathrm{d} z$</li>
</ul>

<h3 id="vectorizing-with-python">Vectorizing with Python</h3>

<ul>
  <li>Avoid using “for-loop” as much as possible</li>
  <li>Make sure that the data are matrix rather than array, or the $\texttt{broadcasting}$ in Python may causes bugs</li>
</ul>

<h1 id="week-3">Week 3</h1>

<p>An example of a 4-layer NN:</p>

<ul>
  <li>Layer 0: the input layer (not counted), $x$</li>
  <li>Layer 1 to 3: the hidden layers</li>
  <li>Layer 4: the output layer, $\hat{y}$</li>
</ul>

<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggpg4hpujfj319s0oewts.jpg" alt="4L" /></p>

<h3 id="two-layer-neural-network">Two-Layer Neural Network</h3>

<h4 id="structure-of-an-l1-layer-nn">Structure of an $l+1$ layer NN</h4>

<ol>
  <li>Input layer: features $X$, or $a^{[0]}$, <strong>NOT</strong> counted as a layer
    <ul>
      <li>$X \in \mathbb{R}^{n_x \times m}$</li>
    </ul>
  </li>
  <li>Hidden layer. A neuron is composed by:
    <ul>
      <li>A linear function, $Z$, with parameters $W$ and $b$: $Z^{[l]} = W \times X + b$</li>
      <li>A non-linear activation function, $A$, $A^{[l]} = g(Z^{[l]})$</li>
    </ul>
  </li>
  <li>Output layer: $\hat{Y}$, counted as layer $l+1$, or $a^{[l+1]}$</li>
</ol>

<h4 id="notations">Notations</h4>

<p>In the case of $m$ training examples, each of which has $n_x$ features, and the 1st layer has $I$ nodes:</p>

<ul>
  <li>
    <p>The $j$-th training example, $x^{(j)}$ is a column vector, and $x^{(j)} \in \mathbb{R}^{n_x \times 1}$</p>

    <ul>
      <li>Stacking the $m$ examples, get $X \in \mathbb{R}^{n_x \times m}$</li>
    </ul>
  </li>
  <li>
    <p>$a^{[l](j)}_i$: the i-th node of the j-th trainning example in the l-th layer</p>

    <ul>
      <li>Taking the $i$-th node in the 1st layer for example:
        <ul>
          <li>$a^{[1]}_i=g(z^{[1]}_i)$</li>
          <li>$z^{[1]}_i = w^{[1]T}_i X + b^{[1]}_i$,  where  $w^{[1]}_i = w^{[1](j)}_i \in \mathbb{R}^{n_x \times 1}$, $b^{[1]}_i \in \mathbb{R}$ ($\texttt{braodcasting}$!)</li>
        </ul>
      </li>
      <li>
        <p>Stacking all the nodes:</p>

        <ul>
          <li>$W^{[1]}=\begin{bmatrix} \dots &amp; w^{[1]T}_1 &amp; \dots \\ \ddots &amp; \vdots &amp; \ddots \\ \dots &amp; w^{[1]T}_I &amp; \dots  \end{bmatrix}_{I \times n_x}$</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>$b^{[1]} \in \mathbb{R}^{I \times 1}$
    <ul>
      <li>$Z^{[1]} = W^{[1]}X+b^{[1]}$, $Z^{[1]} \in \mathbb{R}^{I \times m}$</li>
      <li>$A^{[1]} = g(Z^{[1]})$</li>
    </ul>
  </li>
  <li>Then the 2nd layer:
    <ul>
      <li>$Z^{[2]} = W^{[2]}a^{[1]}+b^{[2]}$</li>
      <li>$a^{[2]} = g(Z^{[2]})$</li>
    </ul>
  </li>
</ul>

<h3 id="activation-function">Activation Function</h3>

<h4 id="functions-usually-used-as-activation">Functions usually used as activation:</h4>

<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Sigmoid function</th>
      <th>Hyperbolic tangent (tanh) function</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Expression</td>
      <td>$\sigma(z) = \frac{1}{1+e^{-z}}$</td>
      <td>$\sigma(z) = \frac{1}{1+e^{-z}}$</td>
    </tr>
    <tr>
      <td>Plot</td>
      <td><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggn35rx5dcj30yw0naq5v.jpg" alt="Sigmoid" style="zoom: 30%;" /></td>
      <td><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggn35rx5dcj30yw0naq5v.jpg" alt="Sigmoid" style="zoom: 30%;" /></td>
    </tr>
    <tr>
      <td><strong>Name</strong></td>
      <td><strong>Rectify linear unit (ReLU) function</strong></td>
      <td><strong>Leaky ReLu function</strong></td>
    </tr>
    <tr>
      <td>Expression</td>
      <td>$g(z) = \mathrm{max}\{0,z\}$</td>
      <td>e.g., $g(z) = \mathrm{max}\{0.01z,z\}$</td>
    </tr>
    <tr>
      <td>Plot</td>
      <td><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggn3jjqblwj30t80gijsp.jpg" alt="ReLU" style="zoom:33%;" /></td>
      <td><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggn3q1ukgmj31bm0jy0vj.jpg" alt="LReLU" style="zoom:33%;" /></td>
    </tr>
  </tbody>
</table>

<h4 id="suggestions-on-choosing-the-activation-function">Suggestions on choosing the activation function</h4>

<ol>
  <li>Sigmoid function is <strong>rarely</strong> used in the hidden layer;</li>
  <li>Sigmoid function <strong>may</strong> be used for the <strong>output</strong> layer for <strong>binary classification</strong></li>
  <li>Tanh function always performs better and trains faster than sigmoid function in the hidden layers</li>
  <li>ReLU function is the <strong>most</strong> commonly used function, it also trains faster</li>
  <li>A leaky ReLU function works better than ReLU function, but it is not commonly used, and ReLU is usually good enough</li>
  <li><strong>Then just use ReLU : )</strong></li>
</ol>

<h4 id="why-there-have-to-be-a-non-linear-activation">Why there have to be a non-linear activation</h4>

<ul>
  <li>A combination of linear function is still a linear function</li>
  <li>Only for <strong>regression problems</strong>, the <strong>output</strong> layer might use a linear activation func</li>
</ul>

<h3 id="implement-the-gradient-descent">Implement the Gradient Descent</h3>

<h4 id="forward-propagation">Forward propagation</h4>

<p>Just calculate from the input layer through the hidden layer to the output layer <strong>using MATRIX !!!</strong></p>

<h4 id="backpropagation">Backpropagation</h4>

<blockquote>
  <p>A good tutorial <a href="https://youtu.be/oqd4PXZGnL4">here</a></p>
</blockquote>

<ul>
  <li>
    <p>Use the chain rule</p>
  </li>
  <li>
    <p>Keep the dimensions matched</p>

    <p><img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ggndsojyglj31hc0u07wh.jpg" alt="BP" /></p>
  </li>
</ul>

<h4 id="random-initialization">Random Initialization</h4>

<p>To avoid symmetric (doing exactly the same calculation for all neurons)</p>

<table>
  <tbody>
    <tr>
      <td>Always start small. Most of the activation function has flat slope at large $</td>
      <td>x</td>
      <td>$, leading to slow learning</td>
    </tr>
  </tbody>
</table>

<h1 id="week-4">Week 4</h1>

<h3 id="forward-propagation-wrap-up">Forward Propagation wrap-up</h3>

<h4 id="forward-propagation-1">Forward Propagation</h4>

<ul>
  <li>
    <p><strong>for one training set:</strong></p>

    <ul>
      <li>
        <p>$z^{[l]} = w^{[l]}\times a^{[l-1]}+b^{[l]}$</p>
      </li>
      <li>
        <p>$a^{[l]} = g^{[l]}(z^{[l]})$</p>
      </li>
    </ul>
  </li>
  <li>
    <p><strong>for the whole training set:</strong></p>

    <ul>
      <li>$Z^{[l]} = W^{[l]}\times A^{[l-1]}+b^{[l]}$</li>
    </ul>
  </li>
  <li>$A^{[l]} = g^{[l]}(Z^{[l]})$
    <ul>
      <li>where $Z^{[1]}=\begin{bmatrix} \vdots &amp; \vdots &amp; \vdots \\  z^{[l](1)} &amp; \vdots &amp; z^{[l](m)} \\ \vdots &amp; \vdots &amp; \vdots \end{bmatrix}_{I \times n_x}$</li>
    </ul>
  </li>
  <li>$A^{[0]} = X$, and $A^{[L]} = \hat{Y}$</li>
</ul>

<h4 id="backpropagation-1">Backpropagation</h4>

<p>$\mathrm{d}Z^{[L]}=A^{[L]}-Y$</p>

<p>$\mathrm{d}W^{[L]} = \frac{1}{m} \mathrm{d} z^{[L]} A^{[L-1]T}$</p>

<p>$\mathrm{d}b^{[L]} = \frac{1}{m}\mathrm{np.sum}(\mathrm{d}Z^{[L]}, \mathrm{axis} = 1, \mathrm{keepdims} = True)$</p>

<p>$\mathrm{d}Z^{[L-1]} = \mathrm{d}W^{[L]T} \mathrm{d}Z^{[L]} *\operatorname{g}’^{[L-1]}(Z^{[L-1]})$</p>

<p>…</p>

<p>$\mathrm{d}Z^{[1]} = \mathrm{d}W^{[2]} \mathrm{d}Z^{[2]} \operatorname{g}’^{[1]}(Z^{[1]})$</p>

<p>$\mathrm{d}W^{[1]} = \frac{1}{m} \mathrm{d} z^{[1]} A^{[0]T}$</p>

<p>$\mathrm{d}b^{[1]} = \frac{1}{m}\mathrm{np.sum}(\mathrm{d}Z^{[1]}, \mathrm{axis} = 1, \mathrm{keepdims} = True)$</p>

<blockquote>
  <p>”/*” denote <strong>element-wise</strong> multiplication</p>
</blockquote>

<blockquote>
  <p>During the forward &amp; backward computations, many quantities are use repeatedly. Thus it is more efficient to “catch” these quantities for latter uses, such as $w^{[l]}$, $b^{[l]}$, $a^{[l]}$, $\mathrm{d}w^{[l]}$, $\mathrm{d}b^{[l]}$, and $\mathrm{d}a^{[l]}$</p>
</blockquote>

<h3 id="get-the-matrix-dimension-right">Get the Matrix Dimension Right</h3>

<blockquote>
  <p>A great tool for debugging!</p>
</blockquote>

<ul>
  <li>$w^{[l]}$, $\mathrm{d}{w}^{[l]}$: $(n^{[l]}, n^{[l-1]})$</li>
  <li>$b^{[l]}$, $\mathrm{d}{b}^{[l]}$: $(n^{[l]}, 1)$</li>
  <li>$z^{[l]}$, $a^{[l]}$, $dz^{[l]}$, $da^{[l]}$: $(n^{[l]}, 1)$</li>
  <li>$Z^{[l]}$, $A^{[l]}$, $dZ^{[l]}$, $dAa^{[l]}$: $(n^{[l]},m)$</li>
  <li>because of $\mathtt{braodcasting}$, $B^{[l]}$ is the same as $b^{[l]}$</li>
</ul>

<h3 id="why-use-a-deep-nn">Why Use a Deep NN?</h3>

<ul>
  <li>Can handle simple to complicated structures</li>
  <li>A function that can be compute with a “small” L-layre deep NN, may require exponentially more hidden units in a shallow NN</li>
</ul>

<hr />

<h1 id="practice">Practice</h1>

<h2 id="week-2-logistic-regression-with-a-neural-network-mindset">Week 2. Logistic Regression with a Neural Network Mindset</h2>

<h3 id="1-prepare-the-data">1. Prepare the data</h3>

<p>Flatten, reshape, and standardize the data.</p>

<p>For RGB image data, just divide 225.</p>

<h3 id="2-main-steps-of-building-a-nn">2. Main steps of building a NN</h3>

<ol>
  <li>Define the model structure (such as number of input features)</li>
  <li>Initialize the model’s parameters</li>
  <li>Loop:
    <ol>
      <li>Calculate current loss (forward propagation)</li>
      <li>Calculate current gradient (backward propagation)</li>
      <li>Update parameters (gradient descent)</li>
    </ol>
  </li>
</ol>

<p>The above steps are often built separately and integrated into <strong>one function</strong>, the <strong>$\mathtt{model()}$</strong></p>

<p>The functions for the model may including:</p>

<ul>
  <li>activation function</li>
  <li>initializing function: initialize the weightings and bias with zeros</li>
  <li>propagate function: forward and backward propagation, with cost</li>
  <li>optimize function</li>
  <li>prediction function: use the optimized results to predict</li>
</ul>

<p>And put the above functions in the model:</p>

<ol>
  <li>Initialization</li>
  <li>Gradient descent</li>
  <li>Retrieve parameters w and b</li>
  <li>Predict the training and test set</li>
</ol>

<h3 id="further-analysis">Further analysis</h3>

<ol>
  <li>choose the learning rate. can plot the learning curve for reference</li>
</ol>


                <hr style="visibility: hidden;">
                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2020/07/19/test/" data-toggle="tooltip" data-placement="top" title="title">
                        Previous<br>
                        <span>title</span>
                        </a>
                    </li>
                    
                    
                </ul>
                <hr style="visibility: hidden;">

                
                <!-- disqus 评论框 start -->
                <div class="comment">
                    <div id="disqus_thread" class="disqus-thread"></div>
                </div>
                <!-- disqus 评论框 end -->
                

                
            </div>  

    <!-- Side Catalog Container -->
        
            <div class="
                col-lg-2 col-lg-offset-0
                visible-lg-block
                sidebar-container
                catalog-container">
                <div class="side-catalog">
                    <hr class="hidden-sm hidden-xs">
                    <h5>
                        <a class="catalog-toggle" href="#">CATALOG</a>
                    </h5>
                    <ul class="catalog-body"></ul>
                </div>
            </div>
        

    <!-- Sidebar Container -->
            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                


<section>
    
        <hr class="hidden-sm hidden-xs">
    
    <h5><a href="/archive/">FEATURED TAGS</a></h5>
    <div class="tags">
        
        
        
        
        
    </div>
</section>


                <!-- Friends Blog -->
                
<hr>
<h5>FRIENDS</h5>
<ul class="list-inline">
  
  <li><a href=""></a></li>
  
</ul>

            </div>
        </div>
    </div>
</article>

<!-- add support for mathjax by voleking-->

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    TeX: {
      equationNumbers: {
        autoNumber: "AMS"
      }
    },
    SVG: {
      scale: 90
    },
    tex2jax: {
      inlineMath: [ ['$','$'] ],
      displayMath: [ ['$$','$$'] ],
      processEscapes: true,
    }
  });
</script>
<script type="text/javascript"
        src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG">
</script>






<!-- disqus 公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = "YuweiAstro";
    var disqus_identifier = "/2020/07/20/DL-course1";
    var disqus_url = "http://localhost:4000/2020/07/20/DL-course1/";

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<!-- disqus 公共JS代码 end -->




<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>
<!-- anchor-js, Doc:http://bryanbraun.github.io/anchorjs/ -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/anchor-js/1.1.1/anchor.min.js",function(){
        anchors.options = {
          visible: 'hover',
          placement: 'right',
          // icon: '#'
        };
        anchors.add().remove('.intro-header h1').remove('.subheading').remove('.sidebar-container h5');
    })
</script>
<style>
    /* place left on bigger screen */
    @media all and (min-width: 800px) {
        .anchorjs-link{
            position: absolute;
            left: -0.75em;
            font-size: 1.1em;
            margin-top : -0.1em;
        }
    }
</style>



    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <!-- SNS Link -->
                


<ul class="list-inline text-center">


  
  
  
  
  
  
  <li>
    <a target="_blank" href="https://github.com/yuweiastro">
      <span class="fa-stack fa-lg">
        <i class="fa fa-circle fa-stack-2x"></i>
        <i class="fa fa-github fa-stack-1x fa-inverse"></i>
      </span>
    </a>
  </li>
  
  
</ul>

                <p class="copyright text-muted">
                    Copyright &copy; Yuwei Blog 2020
                    <br>
                    Powered by <a href="http://huangxuan.me">Hux Blog</a> |
                    <iframe style="margin-left: 2px; margin-bottom:-5px;" frameborder="0" scrolling="0" width="100px"
                        height="20px"
                        src="https://ghbtns.com/github-btn.html?user=huxpro&repo=huxpro.github.io&type=star&count=true">
                    </iframe>
                </p>
            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js "></script>

<!-- Bootstrap Core JavaScript -->
<!-- Currently, only navbar scroll-down effect at desktop still depends on this -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/hux-blog.min.js "></script>

<!-- Simple Jekyll Search -->
<script src="/js/simple-jekyll-search.min.js"></script>

<!-- Service Worker -->

<script src="/js/snackbar.js "></script>
<script src="/js/sw-registration.js "></script>


<!-- async load function -->
<script>
    function async(u, c) {
        var d = document, t = 'script',
            o = d.createElement(t),
            s = d.getElementsByTagName(t)[0];
        o.src = u;
        if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
        s.parentNode.insertBefore(o, s);
    }
</script>

<!--
     Because of the native support for backtick-style fenced code blocks
     right within the Markdown is landed in Github Pages,
     From V1.6, There is no need for Highlight.js,
     so Huxblog drops it officially.

     - https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0
     - https://help.github.com/articles/creating-and-highlighting-code-blocks/
     - https://github.com/jneen/rouge/wiki/list-of-supported-languages-and-lexers
-->
<!--
    <script>
        async("http://cdn.bootcss.com/highlight.js/8.6/highlight.min.js", function(){
            hljs.initHighlightingOnLoad();
        })
    </script>
    <link href="http://cdn.bootcss.com/highlight.js/8.6/styles/github.min.css" rel="stylesheet">
-->







<!--fastClick.js -->
<script>
    async("//cdnjs.cloudflare.com/ajax/libs/fastclick/1.0.6/fastclick.min.js", function () {
        var $nav = document.querySelector("nav");
        if ($nav) FastClick.attach($nav);
    })
</script>


<!-- Google Analytics -->



<!-- Baidu Tongji -->

<script>
    // dynamic User by Hux
    var _baId = 'e0d2400646dc6f0dfb5b1300b4163306';

    // Originial
    var _hmt = _hmt || [];
    (function () {
        var hm = document.createElement("script");
        hm.src = "//hm.baidu.com/hm.js?" + _baId;
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
    })();
</script>



<!-- Side Catalog -->

<script type="text/javascript">
    function generateCatalog(selector) {

        // interop with multilangual 
        if ('' == 'true') {
            _containerSelector = 'div.post-container.active'
        } else {
            _containerSelector = 'div.post-container'
        }

        // init
        var P = $(_containerSelector), a, n, t, l, i, c;
        a = P.find('h1,h2,h3,h4,h5,h6');

        // clean
        $(selector).html('')

        // appending
        a.each(function () {
            n = $(this).prop('tagName').toLowerCase();
            i = "#" + $(this).prop('id');
            t = $(this).text();
            c = $('<a href="' + i + '" rel="nofollow">' + t + '</a>');
            l = $('<li class="' + n + '_nav"></li>').append(c);
            $(selector).append(l);
        });
        return true;
    }

    generateCatalog(".catalog-body");

    // toggle side catalog
    $(".catalog-toggle").click((function (e) {
        e.preventDefault();
        $('.side-catalog').toggleClass("fold")
    }))

    /*
     * Doc: https://github.com/davist11/jQuery-One-Page-Nav
     * Fork by Hux to support padding
     */
    async("/js/jquery.nav.js", function () {
        $('.catalog-body').onePageNav({
            currentClass: "active",
            changeHash: !1,
            easing: "swing",
            filter: "",
            scrollSpeed: 700,
            scrollOffset: 0,
            scrollThreshold: .2,
            begin: null,
            end: null,
            scrollChange: null,
            padding: 80
        });
    });
</script>



<!-- Multi-Lingual -->


<!-- Simple Jekyll Search -->
<script>
    // https://stackoverflow.com/questions/1912501/unescape-html-entities-in-javascript
    function htmlDecode(input) {
        var e = document.createElement('textarea');
        e.innerHTML = input;
        // handle case of empty input
        return e.childNodes.length === 0 ? "" : e.childNodes[0].nodeValue;
    }

    SimpleJekyllSearch({
        searchInput: document.getElementById('search-input'),
        resultsContainer: document.getElementById('search-results'),
        json: '/search.json',
        searchResultTemplate: '<div class="post-preview item"><a href="{url}"><h2 class="post-title">{title}</h2><h3 class="post-subtitle">{subtitle}</h3><hr></a></div>',
        noResultsText: 'No results',
        limit: 50,
        fuzzy: false,
        // a hack to get escaped subtitle unescaped. for some reason, 
        // post.subtitle w/o escape filter nuke entire search.
        templateMiddleware: function (prop, value, template) {
            if (prop === 'subtitle' || prop === 'title') {
                if (value.indexOf("code")) {
                    return htmlDecode(value);
                } else {
                    return value;
                }
            }
        }
    });

    $(document).ready(function () {
        var $searchPage = $('.search-page');
        var $searchOpen = $('.search-icon');
        var $searchClose = $('.search-icon-close');
        var $searchInput = $('#search-input');
        var $body = $('body');

        $searchOpen.on('click', function (e) {
            e.preventDefault();
            $searchPage.toggleClass('search-active');
            var prevClasses = $body.attr('class') || '';
            setTimeout(function () {
                $body.addClass('no-scroll');
            }, 400)

            if ($searchPage.hasClass('search-active')) {
                $searchClose.on('click', function (e) {
                    e.preventDefault();
                    $searchPage.removeClass('search-active');
                    $body.attr('class', prevClasses);  // from closure 
                });
                $searchInput.focus();
            }
        });
    });
</script>


<!-- Image to hack wechat -->
<img src="/img/icon_wechat.png" width="0" height="0" />
<!-- Migrate from head to bottom, no longer block render and still work -->

</body>

</html>
