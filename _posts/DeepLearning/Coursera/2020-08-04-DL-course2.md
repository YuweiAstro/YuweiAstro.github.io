---
layout:     post
title:      "「Note」Improving Deep Neural Networks"
subtitle:   "Coursera Deep Learning Specialization -- Course 2"
date:       2020-08-04
author:     "Yuwei"
header-img: "img/post-bg-DL2.jpg"
header-mask: 0.5
mathjax: true
category: [deep-learning, coursera]
tags:
  - deep learning
  - Coursera
  - note
---

# Week 1. Hyperparameter tuning, Regularization and Optimization

## 1. Data

**Divide all the data into three parts:**

- the training set: to train the NN
- the develop (dev) set: to tune the model
- the test set: to test the model's performance (don't tune the model according to its results, that's what dev set is for)

**Empirical division strategy:**

Small data: 0.6, 0.2, 0.2

Big data: 0.98, 0.01, 0.01 (e.g.)

⚠️ The data sets should have the same distribution. E.g. an classifier may be trained by high-resolution images but the dev & test data sets are.

## 2. Bias & Variance

#### Bias & Variance

**Bias:** the difference between the estimator's expected value and the true value of hte parameter being estimated ([Ref](https://en.wikipedia.org/wiki/Bias_of_an_estimator)). **High bias $\rightarrow$ under fitting**

**Variance:** an error from sensitivity to small fluctuations in the training set ([Ref](https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff)). **High variance $\rightarrow$ over fitting**

High bias and high variance can coexist, especially for multi--dimension data.

**Bias--Variance Tradeoff**

- Cross--Validation
- or mean squared error (MSE)

#### Optimal Error / Bayes Error

**Def:** the error of an *ideal* model, or the lowest possible prediction error that can be achieved ([Ref](https://stats.stackexchange.com/questions/302900/what-is-bayes-error-in-machine-learning))

Used as a reference for evaluating the performance of a model.

## 3. How to Deal with Bias and Variance

**If high bias (from training set performance):**

- Bigger NN
- Longer training
- Other NN architecture

**If high variance (from dev set performance):**

- More data
- Regularization
- Other NN architecture

## 4. Regularization

> To address the problem of overfitting
>
> Regularization works well when there are lots of slightly useful features.

#### The Formula

Hyper parameter $\lambda$, the regularization parameter.

**The updated cost function:**

$$J(w,b) = \frac{1}{m} \sum\limits_{i=1}^{m} L(\hat{y}^{(i)}, y^{(i)}) + \frac{\lambda}{2m} \sum\limits_{l=1}^L \| w^{[l]} \|^2_2$$,

in which $\| w^{[l]} \|^2_2 = \sum\limits_{i=1}^{n^l}\sum\limits_{j=1}^{n^{[l-1]}} (w_{i,j}^{[l]})^2$ is the L2 regularization, and the value is the squared Euclidean norm; the matrix norm  is called the "Frobenius norm".

Also, the **updated derivatives**:

$$\mathrm{d}w^{[l]} = \frac{1}{m} \mathrm{d} z^{[l]} a^{[l-1]T} + \frac{\lambda}{m}w^{[l]}$$

$$w^{[l]} := (1-\frac{\alpha \lambda}{m}) w^{[l]} - \frac{\alpha}{m} \mathrm{d} z^{[l]} a^{[l-1]T}$$

#### Why It Works

Since we aim to decrease the cost function,  adding the regularization item penalizes the parameter w from being too large. 

## 5. Dropout

>  To address the problem of overfitting
>
>  Always used in CV

**Dropout is only used for the test set.**

**Basic idea:**

Randomly eliminate neurons  in the **hidden layers** at **each** iteration.

#### Implementation

Common method: **inverted dropout**

- Set the **hyper parameter "keep probability"**, it may be difference for different layers
- Generate a random matrix with the same shape of the layer
- set 1 to items higher than the keep-prob, 0 to the others
- multiply the random matrix with the original neuron matrix A
- Set A = A/keep-prob to keep the same expectation

#### Why It Works

Equivalent to fitting with a smaller NN.

The model will spread the weights on the neurons in case some of them are eliminated

## 6. Other Regularization Methods

> To address the problem of overfitting

1. Enlarge the data. 

   e.g. for images, flip/random distortion the images to create "fake examples"

2. Early stopping

   Stop training at the iteration with best performance (lowest J)

   Drawbacks: optimizing J and doing regularization at the same time, not up to the orthogonalization method (think about one task at a time), thus cannot solve the problems separately.

## 7. Optimization

#### 1. Normalize the Training Set

$x := \frac{x-\mu}{\sigma}$, where $\mu = \frac{1}{m} \sum \limits_{i=1}^{m}x^{(i)}$, $\sigma^2 = \frac{1}{m}  \sum \limits_{i=1}^{m}x^{(i)2}$

#### 2. Vanishing/Exploding Gradients

**Def:** the derivatives become extremely large or small when training a deep NN

**Solution:** 

- the larger input features $n$, the smaller $w_i$;

- initialize $w_i$ with $\mathrm{Variance}(w_i) = \frac{1}{n}$, or $\frac{2}{n}$ for ReLU

Implementation notes:

```python
w[l] = np.random.randn(shape_of_w[l]) * np.sqrt(1/n[l-1]) # 2/n[l-1] for ReLU
```

#### 3. Gradient Checking

> To check whether the back--propagation implementation is right.

1. Reshape each $W$, $b$ in to vector $\Theta$; $\mathrm{d}W$, $\mathrm{d}b$ into vector $\mathrm{d}\Theta$
2. Thus $J(W^{[1]}, b^{[1]}, \dots, W^{[i]}, b^{[i]}, \dots, W^{[L]}, b^{[L]}) = J(\Theta) = J(\Theta_1, \dots, \Theta_i, \dots)$ 
3. for each $i$, let $\mathrm{d}\Theta_{approx}^{[i]} = \frac{J(\Theta_1, \dots, \Theta_i + \epsilon, \dots) - J(\Theta_1, \dots, \Theta_i, \dots)}{2\epsilon}$, which is the approximation of $\frac{\partial J}{\partial \Theta_i}$
4. Check if $\frac{\parallel\mathrm{d}\Theta_{approx} - \mathrm{d}\Theta \parallel_2}{\parallel\mathrm{d}\Theta_{approx} \parallel_2 + \parallel\mathrm{d}\Theta \parallel_2} < \epsilon = 10^{-7}$
   - $\sim 10^{-7}$: great
   - $\sim 10^{-5}$: some components may be too large/have bugs
   - $\sim 10^{-3}$: careful of BUGs!!!

**Implementation  Notes:**

1. Only use to debug, NOT while training (computational resources consuming).
2. If failed, look at components to identify bug.
3. Remember regularization if used during training.
4. Doesn't work with dropout.
5. Run at random initialization; perhaps again after some training.



# Week 2. Optimization Algorithms

## 1. Mini--batch Gradient Descent

> To speed up the training

Divide the training set into smaller "batches", and update the parameters after one mini-batch is trained.

- batch size = size of training set $\rightarrow$ batch gradient
  - good for small training set (< 2000)
  - smooth curves of  J vs. #iterations plot
  - slow for large training set
- batch size = 1 $\rightarrow$ Stochastic batch (just train one example then update the parameters, and move to the next example)
  - loose speedup from vactorization
  - the curve is extremely noisy
- mini batch usually have size between the above and is the power of 2 (64 ~512)
  - good for large training set
  - the curve is noisier than batch gradient descent, but smoother than Stochastic batch
  - take advantage of the speedup from vectorization
  - make progress without processing the entire training set

## 2. Gradient Descent with Momentum

### 1. Exponentially Weighted Averages

#### Definition

$\theta_t$ is the value, and $v_t$ is the corresponding EWA, then:

- $v_0 = 0$	

- $v_t = \beta v_{t-1} + (1-\beta) \theta_t$, $(\beta < 1)$  

$v_i$ is approximately the average of the $\frac{1}{1-\beta}$ data points;

the curve becomes smoother and shifts to right ( latency) when $\beta$ increases.

<img src="https://tva1.sinaimg.cn/large/007S8ZIlly1ghdziqgjmwj30qs0ean5i.jpg" alt="EWA" style="zoom:50%;" />

#### Intuition

$(1-\epsilon)^{\frac{1}{\epsilon}} = \frac{1}{e} \approx 0.37$

Thus it takes about 1/$\epsilon$ items before the weight decay below $e^{-1}$.

#### Bias Correction

For the initial phase, update the original $v_t$ with $\frac{v_t}{1-\beta^t}$.

Unnecessary for larger t.

Not really matters whether to correct this.

### 2. Gradient Descent with Momentum

> Always works faster than the standard gradient descent algorithm

**Why it works:**

Smooth out the steps of gradient descents perpendicular to the direction towards minimum, but save the progress towards the minimum.

![EWAIntuition](https://tva1.sinaimg.cn/large/007S8ZIlly1ghe0eapsisj31ce09ownf.jpg)

Use EWA to update the gradient. **Hyperparameter, $\beta$**

**Implementation:**

- Initialize $v_{\mathrm{d} w} = 0$, and $v_{\mathrm{d} b} = 0$

- On iteration of t:
  - compute $\mathrm{d}w$, $\mathrm{d}b$ on current mini--batch
  - compute $v_{\mathrm{d} w} = \beta v_{\mathrm{d}w} + (1-\beta) \mathrm{d}w$; $v_{\mathrm{d} b} = \beta v_{\mathrm{d}b} + (1-\beta) \mathrm{d}b$

  - update $w$ with $w := w - \alpha v_{\mathrm{d} w}$ and $b := b - \alpha v_{\mathrm{d} b}$

Often choose $\beta = 0.9$ 

## 3. Root--Mean--Square Prop (RMSprop)

**Implementation:**

- On iteration of t:
  - compute $\mathrm{d}w$, $\mathrm{d}b$ on current mini--batch
  - compute $S_{\mathrm{d} w} = \beta S_{\mathrm{d}w} + (1-\beta) \mathrm{d}w^2$; $S_{\mathrm{d} b} = \beta S_{\mathrm{d}b} + (1-\beta) \mathrm{d}b^2$ (**element wise squaring**)
  - update $w$ with $w := w - \alpha \frac{\mathrm{d} w}{\sqrt{S_{\mathrm{d} w}}}$ and $b := b -  \alpha \frac{\mathrm{d} b}{\sqrt{S_{\mathrm{d} b}}}$

⚠️ To avoid divided by zero, always use $\sqrt{S_{\mathrm{d} b}} + \epsilon$ ($\epsilon = 10^{-8}$) instead of just $\sqrt{S_{\mathrm{d} b}}$

**Why it works:**

Similar to the gradient descent with momentum.

## 4. Adaptive Moment Estimation (Adam) Optimization Algorithm

> A combination of momentum and RMSprop

> Very efficient!

**Implementation:**

- Initialize $v_{\mathrm{d} w} $, $v_{\mathrm{d} b}$, $S_{\mathrm{d} w} $, and $S_{\mathrm{d} b}$ to zero
- On iteration of t:
  - compute $\mathrm{d}w$, $\mathrm{d}b$ on current mini--batch
  - compute momentum with $\beta_1$: $v_{\mathrm{d} w} = \beta_1 v_{\mathrm{d}w} + (1-\beta_1) \mathrm{d}w$; $v_{\mathrm{d} b} = \beta_1 v_{\mathrm{d}b} + (1-\beta_1) \mathrm{d}b$
  - compute the RMSprop with $\beta_2$: $S_{\mathrm{d} w} = \beta_2 S_{\mathrm{d}w} + (1-\beta_2) \mathrm{d}w^2$; $S_{\mathrm{d} b} = \beta_2 S_{\mathrm{d}b} + (1-\beta_2) \mathrm{d}b^2$
  - bias correction: $v_{\mathrm{d} w}^{corrected} = \frac{v_{\mathrm{d} w}}{1-\beta_1}$, $v_{\mathrm{d} b}^{corrected} = \frac{v_{\mathrm{d} b}}{1-\beta_1}$, $S_{\mathrm{d} w}^{corrected} = \frac{v_{\mathrm{d} S}}{1-\beta_2}$, and $S_{\mathrm{d} b}^{corrected} = \frac{v_{\mathrm{d} b}}{1-\beta_2}$
  - perform the update: $w := w - \alpha \frac{v_{\mathrm{d} w}^{corrected}}{\sqrt{S_{\mathrm{d} w}}+\epsilon}$ and $b := b - \alpha \frac{b_{\mathrm{d} b}^{corrected}}{\sqrt{S_{\mathrm{d} b}}+\epsilon}$

**Hyperparameters choice:**

- learning rate $\alpha$: needs to be tuned
- momentum $\beta_1$: 0.9
- RMSprop $\beta_2$: 0.999
- $\epsilon$: 10$^{-8}$

## 5. Learning Rate Decay

To decrease the learning rate when closing to the minimum.

> an epoch: one pass through the data (all the mini--batches)

Set the learning rate $\alpha = $ 

- $\frac{1}{1 + decay-rate \times epoch_number} \alpha_0$

- or  e.g., $0.95^{epoch-num} \alpha_0$
- or $\frac{k}{\sqrt{epoch-num}} \alpha_0$ ($k$ is a constant)
- or manually tuned

## 6. The Problem of Local Optima

Not really need to worry, especially for high-dimension data.



# Week 3

## 1. Hyperparameter Tuning

**Priority:**

1.  Learning rate $\alpha$, momentum parameter $\beta_1$, RMSprop parameter $\beta_2$
2. Number of hidden units
3. Number of layers, learning-rate decay parameter

**How to:**

1. Try random parameter sets instead of gridded

   Gridded parameters means the others remain unchanged when tuning one parameter, which is a waste of time.

2. Sample more sets of values near the one with best performance

**Sampling:**

For values have a span across several scales, e.g. 0.001$\sim$0.1, 0.9$\sim$0.999, linear sampling will cause the sampled value thick at the large scale and thin at the small scale.

**Solution**: sample in **log-space**. e.g. sample 0.001$\sim$0.1 at the log-space of $[-3, -1]$, then use $10^a$; re-write 0.9$\sim$0.999 as $(1-0.1)\sim(1-0.001)$, and sample $[0.1, 0.001]$ at the log-space of $[-1, -3]$, then use $1-10^a$.

**Training Strategy:**

- "Pandas": keep track of one model, and tune the parameters while training
- "Caviar": try several models at the same time, and pick the best one (when you can access large amount of computational resources)

## 2. Batch Normalization (BN)

**Idea:** normalize the inputs of each layer to train $w, b$ at that layer faster

Usually, normalize **before** activation function, i.e. $z$

**Implementation:**

> in one mini--batch

- Given the intermediate values  in NN, e.g. $Z^{[l]}$:
  - $\mu = \frac{1}{m} \sum \limits_i z^{(i)}$
  - $\sigma^2 = \frac{1}{m}\sum \limits_i (z^{(i)} - \mu)^2$

- Thus $z^{(i)}_{norm} = \frac{z^{(i)}-\mu}{\sigma^2 + \epsilon}$
- Let $\tilde{z}^{(i)} = \gamma z_{norm}^{(i)} + \beta$, in which $\gamma$ and $\beta$ are **learnt** parameters, and are updated similar to and at the same time with the other parameters
  - Note when $\gamma = \sqrt(\sigma ^2+ \epsilon)$ and $\beta = \mu$, $\tilde{z}^{(i)} = z^{(i)}$
  - $b$ is canceled out during the calculation of $z_{norm}$ and $\tilde{z}$, thus just **don't bother to add/update $b$** during the calculation

- Then $a^{[l]} = g^{[l]} (\widetilde{Z}^{[l]})$
- (Loop the above steps towards the output layer, notice that $\gamma$ and $\beta$ are difference in different layer, i.e. should be  $\gamma^{[l]}$ and $\beta^{[l]}$ )

**Why it works:**

When the input X changes, even if the function mapping from X to Y does not change, the distribution of Y also changes and the model need to be retrained (**covariate shift**). By BN, we make sure the inputs of each layer have little influence on the following inputs distribution.

**On Test Set:**

Estimate $\mu$ and $\sigma^2$ with the exponentially weighted average across the mini-batches.

Then use the averaged  $\mu$ and $\sigma^2$, and the learnt $\gamma$ and $\beta$ for calculate $Z_{norm}$ and $\widetilde{Z}$ of the test set.

## 3. Multi--class Classification ---- Softmax Regression

Output layer: a (C, 1) vector of C possibilities (C is the number of classes) instead of 0 or 1.

Reduce to logistic regression when C=2

**Softmax activation function for the output layer:**

- Use a temporary variable t, $t = e^{z^{[L]}}$, in which t, Z are (C, 1) vectors
- $a^{[L]} = \frac{t}{\sum \limits^C_{i=1} t_i}$

Note, the decision boundary between any two classes is **linear**.

**Loss Function:**

$$L(\hat{y}, y) = - \sum \limits^C_{j=1} y_j \log{\hat{y}_j}$$

Note y is a vector with one 1 and $C-1$ zeros, to minimize the loss function, the model has to maximize the item in $\hat{y}$ that is at the same place of the "1" in $y$.

**Cost Function:**

$$J = \frac{1}{m} \sum \limits_{i=1}^m L(\hat{y}^{(i)}, y^{(i)})$$

**Backpropagation:**

$$\mathrm{d}z^{[L]} = \hat{y}-y$$



# Practice

